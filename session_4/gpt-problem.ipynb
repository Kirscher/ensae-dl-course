{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7851040,"sourceType":"datasetVersion","datasetId":4604175}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Mini-GPT implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\n# final results with this code\n# step=4800: train loss = 1.0883, val loss = 1.4908\n# LARTIUS:\n# Now Mars, they love me.\n\n# LARTIUS:\n# Yeven altood my opprovoty.\n\n# MARIANA:\n# Traitor, I will hang\n\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 300\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_heads = 6\nn_layer = 6\ndropout = .2\n\ntorch.manual_seed(1337)\n\nwith open('/kaggle/input/input-txt/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# create vocabulary\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\ndata = torch.tensor(encode(text), dtype=torch.long)\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n    return x, y\n\n@torch.no_grad\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train','val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            _, loss = model(X, Y)\n            losses[k] = loss\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# Copy your Head, MultiHeadAttention, FeedForward and Block classes here\n\nclass Head(nn.Module):\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.K = nn.Linear(n_embd, head_size, bias=False)\n        self.Q = nn.Linear(n_embd, head_size, bias=False)\n        self.V = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # store a persistent buffer for the forward pass\n\n    def forward (self, x):\n        B, T, C = x.shape\n        K = self.K(x)\n        Q = self.Q(x)\n        V = self.V(x)\n        dot_product = Q @ K.transpose(-2, -1) * C**-0.5\n        dot_product = dot_product.masked_fill(self.tril == 0, float('-inf')) # apply the mask\n        S = F.softmax(dot_product, dim=-1)\n        out = S @ V\n        return out\n    \nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.linear = nn.Linear(num_heads * head_size, n_embd, bias=False)\n\n    def forward (self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        return out\n    \nclass FeedForward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n    def __init__(self, n_embd):\n        super().__init__()\n        self.linear1 = nn.Linear(n_embd, 4 * n_embd)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(4 * n_embd, n_embd)\n\n    def forward(self, x):\n        out = self.linear1(x)\n        out = self.relu(out)\n        out = self.linear2(out)\n        return out\n    \nclass Block(nn.Module):\n\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.norm1 = nn.LayerNorm(n_embd)\n        self.norm2 = nn.LayerNorm(n_embd)\n        self.attn = MultiHeadAttention(n_head, head_size)\n        self.ff = FeedForward(n_embd)\n\n    def forward(self, x):\n        out = self.norm1(x)\n        out = self.attn(out)\n        out = out + x\n        out = self.norm2(out)\n        out = self.ff(out)\n        out = out + x\n        return out\n    \nclass GPT(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.pos_embedding_table = nn.Embedding(block_size, n_embd)\n        # define blocks, a layer norm and a linear layer\n        self.blocks = nn.ModuleList([Block(n_embd, n_heads) for _ in range(n_layer)])\n        self.norm = nn.LayerNorm(n_embd)\n        self.linear = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        # (B,T,C)\n        token_emb = self.token_embedding_table(idx)\n        pos_emb = self.pos_embedding_table(torch.arange(T, device=device)) # (T, C)\n        # sum the token embeddings and position embeddings\n        x = token_emb + pos_emb \n        # apply blocks, layer norm and linear layer leading to the logits variable\n        for block in self.blocks:\n            x = block(x)\n        x = self.norm(x)\n        logits = self.linear(x)\n        # do not modify the rest of the method (it computes the loss during the forward pass)\n        if targets is None:\n            loss = None\n        else:\n            # idx and targets are both (B,T) tensor of integers\n            B, T, C = logits.shape\n            logits = logits.view (B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n            # idx is (B, T) array of indices in the current context\n            for _ in range(max_new_tokens):\n                # crop idx to the last block size tokens\n                idx_cond = idx[:, -block_size:]\n                # get the predictions\n                logits, loss = self(idx_cond)\n                # focus only on the last time step\n                logits = logits[:, -1, :] # becomes (B, C)\n                # apply softmax to get probabilities\n                probs = F.softmax(logits, dim=-1) # (B, C)\n                # sample from the distribution\n                idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n                # append sampled index to the running sequence\n                idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n            return idx","metadata":{"_uuid":"9c825014-829f-4519-bc88-a9ba01ffb848","_cell_guid":"a99504c9-62fe-4c7e-9a07-e1dbe458b469","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-03-15T13:08:19.404827Z","iopub.execute_input":"2024-03-15T13:08:19.405242Z","iopub.status.idle":"2024-03-15T13:08:21.597845Z","shell.execute_reply.started":"2024-03-15T13:08:19.405212Z","shell.execute_reply":"2024-03-15T13:08:21.596923Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Training the model","metadata":{}},{"cell_type":"code","source":"model = GPT()\nm = model.to(device)\nprint(m.parameters())\noptimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n\n# training loop\nfor iter in tqdm(range(max_iters)): # increase number of steps for good results... \n    \n    # evaluate once in a while\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print (f\"step={iter}: train loss = {losses['train']:.4f}, val loss = {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-03-15T13:08:39.307630Z","iopub.execute_input":"2024-03-15T13:08:39.308622Z","iopub.status.idle":"2024-03-15T13:39:23.793449Z","shell.execute_reply.started":"2024-03-15T13:08:39.308590Z","shell.execute_reply":"2024-03-15T13:39:23.792495Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"<generator object Module.parameters at 0x7d304dbc6b20>\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/5000 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"step=0: train loss = 4.2764, val loss = 4.2737\n","output_type":"stream"},{"name":"stderr","text":"  6%|▌         | 301/5000 [02:21<12:24:59,  9.51s/it]","output_type":"stream"},{"name":"stdout","text":"step=300: train loss = 2.3162, val loss = 2.3419\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 601/5000 [04:11<11:38:11,  9.52s/it]","output_type":"stream"},{"name":"stdout","text":"step=600: train loss = 1.8838, val loss = 1.9978\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 901/5000 [06:01<10:51:03,  9.53s/it]","output_type":"stream"},{"name":"stdout","text":"step=900: train loss = 1.6250, val loss = 1.8001\n","output_type":"stream"},{"name":"stderr","text":" 24%|██▍       | 1201/5000 [07:50<10:03:47,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=1200: train loss = 1.4831, val loss = 1.6960\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 1501/5000 [09:40<9:16:29,  9.54s/it] ","output_type":"stream"},{"name":"stdout","text":"step=1500: train loss = 1.4000, val loss = 1.6308\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1801/5000 [11:30<8:28:34,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=1800: train loss = 1.3309, val loss = 1.5838\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▏     | 2101/5000 [13:20<7:41:18,  9.55s/it]","output_type":"stream"},{"name":"stdout","text":"step=2100: train loss = 1.2805, val loss = 1.5654\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 2401/5000 [15:10<6:53:02,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=2400: train loss = 1.2354, val loss = 1.5583\n","output_type":"stream"},{"name":"stderr","text":" 54%|█████▍    | 2701/5000 [17:00<6:05:24,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=2700: train loss = 1.1949, val loss = 1.5603\n","output_type":"stream"},{"name":"stderr","text":" 60%|██████    | 3001/5000 [18:49<5:18:08,  9.55s/it]","output_type":"stream"},{"name":"stdout","text":"step=3000: train loss = 1.1539, val loss = 1.5699\n","output_type":"stream"},{"name":"stderr","text":" 66%|██████▌   | 3301/5000 [20:39<4:30:07,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=3300: train loss = 1.1146, val loss = 1.5754\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 3601/5000 [22:29<3:42:21,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=3600: train loss = 1.0614, val loss = 1.5980\n","output_type":"stream"},{"name":"stderr","text":" 78%|███████▊  | 3901/5000 [24:19<2:54:57,  9.55s/it]","output_type":"stream"},{"name":"stdout","text":"step=3900: train loss = 1.0141, val loss = 1.6329\n","output_type":"stream"},{"name":"stderr","text":" 84%|████████▍ | 4201/5000 [26:09<2:07:07,  9.55s/it]","output_type":"stream"},{"name":"stdout","text":"step=4200: train loss = 0.9590, val loss = 1.6885\n","output_type":"stream"},{"name":"stderr","text":" 90%|█████████ | 4501/5000 [27:59<1:19:18,  9.54s/it]","output_type":"stream"},{"name":"stdout","text":"step=4500: train loss = 0.8988, val loss = 1.7746\n","output_type":"stream"},{"name":"stderr","text":" 96%|█████████▌| 4801/5000 [29:48<31:39,  9.54s/it]  ","output_type":"stream"},{"name":"stdout","text":"step=4800: train loss = 0.8373, val loss = 1.8625\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 5000/5000 [30:41<00:00,  2.72it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"### Training took 30:41 for 5000 steps on Google Colab. (1x GPU P100)\n### step=4800: train loss = 0.8373, val loss = 1.8625","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text generation","metadata":{}},{"cell_type":"code","source":"# Examples of results I got (generate 100 tokens) :\n\n# STANGARELY:\n# What they know your about out of odces.\n#\n# KING RICHARD III:\n# Rumour of bite, discover dang\n\n##############\n\n# RARTCHUMTAT:\n# Huntly sir; I had so.\n# \n# TRANIO:\n# Do, no,\n# The stroke is mutiny. People your captain.\n#\n# ARCA\n\n##############\n\n# SIDOABRANLAUS:\n# No, put my soul,\n# To meet thither.\n#\n# ROMEO:\n# And so, I speak,\n# You sad vice say again.\n","metadata":{"execution":{"iopub.status.busy":"2024-03-15T13:52:54.654400Z","iopub.execute_input":"2024-03-15T13:52:54.655307Z","iopub.status.idle":"2024-03-15T13:52:54.659791Z","shell.execute_reply.started":"2024-03-15T13:52:54.655266Z","shell.execute_reply":"2024-03-15T13:52:54.658821Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# generate some text\ncontext = torch.zeros((1, n_embd), dtype=torch.long, device=device)\nprint(decode(m.generate(idx = context, max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-03-15T13:52:22.831365Z","iopub.execute_input":"2024-03-15T13:52:22.832227Z","iopub.status.idle":"2024-03-15T13:52:24.291314Z","shell.execute_reply.started":"2024-03-15T13:52:22.832195Z","shell.execute_reply":"2024-03-15T13:52:24.289955Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSIDOABRANLAUS:\nNo, put my soul,\nTo meet thither.\n\nROMEO:\nAnd so, I speak,\nYou sad vice say again.\n\nP\n","output_type":"stream"}]}]}